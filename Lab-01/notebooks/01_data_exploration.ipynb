{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/01_data_exploration.ipynb\n",
    "# This is a Python script that represents the notebook cells\n",
    "\n",
    "\"\"\"\n",
    "# Cell 1: Setup and Imports\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 2: Load Preprocessed Data\n",
    "\"\"\"\n",
    "# Load vocabulary\n",
    "with open('../data/processed/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# Load train corpus\n",
    "with open('../data/processed/train_corpus.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    train_sequences = train_data['sequences']\n",
    "    train_ids = train_data['ids']\n",
    "\n",
    "# Load validation corpus\n",
    "with open('../data/processed/val_corpus.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "    val_sequences = val_data['sequences']\n",
    "    val_ids = val_data['ids']\n",
    "\n",
    "# Load test corpus\n",
    "with open('../data/processed/test_corpus.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    test_sequences = test_data['sequences']\n",
    "    test_ids = test_data['ids']\n",
    "\n",
    "# Load statistics\n",
    "with open('../data/statistics/corpus_stats.json', 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Train sequences: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences: {len(val_sequences)}\")\n",
    "print(f\"Test sequences: {len(test_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 3: Corpus Statistics Overview\n",
    "\"\"\"\n",
    "print(\"=\" * 60)\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nSequence Counts:\")\n",
    "print(f\"  Training:   {len(train_sequences):,}\")\n",
    "print(f\"  Validation: {len(val_sequences):,}\")\n",
    "print(f\"  Test:       {len(test_sequences):,}\")\n",
    "print(f\"  Total:      {len(train_sequences) + len(val_sequences) + len(test_sequences):,}\")\n",
    "\n",
    "print(f\"\\nToken Statistics:\")\n",
    "print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "print(f\"  Unique tokens: {stats['unique_tokens']:,}\")\n",
    "print(f\"  Vocabulary size: {stats['vocabulary_size']:,}\")\n",
    "print(f\"  OOV tokens: {stats['unique_tokens'] - stats['vocabulary_size']:,}\")\n",
    "\n",
    "print(f\"\\nSequence Length Statistics:\")\n",
    "print(f\"  Average: {stats['avg_sequence_length']:.2f}\")\n",
    "print(f\"  Median: {stats['median_sequence_length']:.0f}\")\n",
    "print(f\"  Min: {stats['min_sequence_length']}\")\n",
    "print(f\"  Max: {stats['max_sequence_length']}\")\n",
    "print(f\"  Std: {stats['std_sequence_length']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 4: Visualize Sequence Length Distribution\n",
    "\"\"\"\n",
    "# Collect all sequence lengths\n",
    "all_lengths = [len(seq) for seq in train_sequences + val_sequences + test_sequences]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(all_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(all_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(all_lengths):.1f}')\n",
    "axes[0].axvline(np.median(all_lengths), color='green', linestyle='--', label=f'Median: {np.median(all_lengths):.1f}')\n",
    "axes[0].set_xlabel('Sequence Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Sequence Lengths')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "data_for_box = {\n",
    "    'Train': [len(seq) for seq in train_sequences],\n",
    "    'Val': [len(seq) for seq in val_sequences],\n",
    "    'Test': [len(seq) for seq in test_sequences]\n",
    "}\n",
    "axes[1].boxplot(data_for_box.values(), labels=data_for_box.keys())\n",
    "axes[1].set_ylabel('Sequence Length')\n",
    "axes[1].set_title('Sequence Length by Dataset Split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23adc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 5: Token Type Distribution\n",
    "\"\"\"\n",
    "# Create pie chart for token types\n",
    "token_types = stats['token_type_distribution']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    token_types.values(), \n",
    "    labels=token_types.keys(),\n",
    "    colors=colors,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90\n",
    ")\n",
    "\n",
    "# Enhance text\n",
    "for text in texts:\n",
    "    text.set_fontsize(12)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_weight('bold')\n",
    "\n",
    "ax.set_title('Token Type Distribution in Corpus', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac29727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cell 6: Most Common Tokens Analysis\n",
    "\"\"\"\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP 30 MOST COMMON TOKENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get more common tokens for analysis\n",
    "all_tokens = []\n",
    "for seq in train_sequences:\n",
    "    all_tokens.extend(seq)\n",
    "\n",
    "token_counter = Counter(all_tokens)\n",
    "most_common_30 = token_counter.most_common(30)\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "tokens = [t[0] for t in most_common_30]\n",
    "counts = [t[1] for t in most_common_30]\n",
    "\n",
    "bars = ax.bar(range(len(tokens)), counts)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_xlabel('Token')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('30 Most Common Tokens in Training Data')\n",
    "\n",
    "# Color code by token type\n",
    "for i, (token, _) in enumerate(most_common_30):\n",
    "    if token in ['<BOS>', '<EOS>', '<UNK>', '<PAD>', '<STRING>', '<NUM>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
